---
phase: 02-error-handling-and-resilience
plan: "02"
type: execute
wave: 2
depends_on:
  - "02-01"
files_modified:
  - src/__tests__/error-scenarios.test.ts
autonomous: true
requirements:
  - TEST-03

must_haves:
  truths:
    - "ENOENT scenario test: handler receives ENOENT CommandExecutionError and surfaces ToolExecutionError with 'not found'/'not installed' in the message — raw 'ENOENT' string does not appear in the MCP response"
    - "Quota scenario test: handler surfaces ToolExecutionError with 'quota exceeded' classification when executeCommand rejects with a quota-pattern error"
    - "Auth scenario test: handler surfaces ToolExecutionError with 'authentication failed' classification when executeCommand rejects with an auth-pattern error"
    - "Timeout scenario test: handler surfaces ToolExecutionError with 'timed out' classification when executeCommand rejects with a timeout error"
    - "Token scrubbing test: scrubTokens() replaces actual env var token values with [REDACTED]; token value does not appear in scrubbed output"
    - "ANSI stripping test: extractResponse() (via handler) strips ANSI sequences from stdout before returning"
    - "All 42+ pre-existing tests continue to pass — no regressions from Plan 02-01 changes"
  artifacts:
    - path: "src/__tests__/error-scenarios.test.ts"
      provides: "ENOENT, quota, auth, timeout, ANSI, token scrubbing test cases"
      contains: "ENOENT"
  key_links:
    - from: "src/__tests__/error-scenarios.test.ts"
      to: "src/errors.ts"
      via: "import scrubTokens for token scrubbing test"
      pattern: "scrubTokens"
    - from: "src/__tests__/error-scenarios.test.ts"
      to: "src/tools/handlers.js (mocked)"
      via: "jest.mock on executeCommand; handler.execute() assertions"
      pattern: "mockedExecuteCommand\\.mockRejectedValue"
---

<objective>
Extend error-scenarios.test.ts with comprehensive tests for every new error-handling path added in Plan 02-01: ENOENT binary-not-found, quota exhaustion, auth failure, timeout, ANSI stripping, and token scrubbing.

Purpose: TEST-03 requires test coverage for all error scenarios. These tests also serve as living documentation of the contract between handlers and error conditions — future regressions will be caught immediately.
Output: Extended error-scenarios.test.ts with 6+ new test cases. All 42+ tests pass.
</objective>

<execution_context>
@/Users/jonathanborduas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jonathanborduas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-error-handling-and-resilience/02-RESEARCH.md
@src/__tests__/error-scenarios.test.ts
@src/errors.ts
@src/tools/handlers.ts
@.planning/phases/02-error-handling-and-resilience/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend error-scenarios.test.ts with Phase 2 error scenario tests</name>
  <files>src/__tests__/error-scenarios.test.ts</files>
  <action>
Extend the existing `src/__tests__/error-scenarios.test.ts` file by adding a new `describe` block for Phase 2 error scenarios. Do NOT modify the existing 7 tests — append after them.

Add these imports at the top of the file (alongside the existing imports):
```typescript
import { CommandExecutionError, scrubTokens } from '../errors.js';
```

Append a new describe block after the closing brace of `describe('Error Handling Scenarios', ...)`:

```typescript
describe('Phase 2: Hardened Error Handling', () => {
  beforeEach(() => {
    mockedExecuteCommand.mockClear();
  });

  // ERR-01: ENOENT — binary not found
  test('AskToolHandler: ENOENT produces user-readable install message, not raw ENOENT', async () => {
    const enoentError = Object.assign(new Error('spawn copilot ENOENT'), { code: 'ENOENT' });
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError(
        'copilot',
        'Binary not found: "copilot". If using the default copilot binary, install it from: https://github.com/github/gh-copilot. If using COPILOT_BINARY_PATH, verify the path is correct.',
        enoentError
      )
    );
    const handler = new AskToolHandler();
    const rejection = handler.execute({ prompt: 'test' });
    await expect(rejection).rejects.toThrow(ToolExecutionError);
    await expect(rejection).rejects.toThrow(/not found|not installed/i);
    // Raw 'ENOENT' string must NOT appear in the ToolExecutionError message
    try {
      await handler.execute({ prompt: 'test' });
    } catch (e) {
      expect((e as Error).message).not.toMatch(/\bENOENT\b/);
    }
  });

  // ERR-02: Quota exhaustion
  test('AskToolHandler: quota error produces "quota exceeded" classification', async () => {
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError('copilot', 'Command failed with exit code 402: quota exceeded', new Error('quota'))
    );
    const handler = new AskToolHandler();
    await expect(handler.execute({ prompt: 'test' })).rejects.toThrow(ToolExecutionError);
    try {
      await handler.execute({ prompt: 'test' });
    } catch (e) {
      expect((e as Error).message).toMatch(/quota exceeded/i);
    }
  });

  // ERR-03: Auth failure
  test('AskToolHandler: auth error produces "authentication failed" classification', async () => {
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError('copilot', 'Command failed with exit code 401: unauthorized', new Error('auth'))
    );
    const handler = new AskToolHandler();
    await expect(handler.execute({ prompt: 'test' })).rejects.toThrow(ToolExecutionError);
    try {
      await handler.execute({ prompt: 'test' });
    } catch (e) {
      expect((e as Error).message).toMatch(/authentication failed/i);
    }
  });

  // ERR-04: Timeout
  test('AskToolHandler: timeout error propagates timed-out message', async () => {
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError('copilot', 'Command timed out after 60000ms', new Error('Timeout'))
    );
    const handler = new AskToolHandler();
    await expect(handler.execute({ prompt: 'test' })).rejects.toThrow(ToolExecutionError);
    try {
      await handler.execute({ prompt: 'test' });
    } catch (e) {
      expect((e as Error).message).toMatch(/timed out/i);
    }
  });

  // SEC-01: Token scrubbing via scrubTokens()
  test('scrubTokens: replaces token value with [REDACTED]', () => {
    const fakeToken = 'ghp_testtoken12345678901234567890123456';
    const original = process.env['GH_TOKEN'];
    process.env['GH_TOKEN'] = fakeToken;
    try {
      const rawMessage = `Authentication failed with token: ${fakeToken}`;
      const scrubbed = scrubTokens(rawMessage);
      expect(scrubbed).not.toContain(fakeToken);
      expect(scrubbed).toContain('[REDACTED]');
    } finally {
      if (original === undefined) {
        delete process.env['GH_TOKEN'];
      } else {
        process.env['GH_TOKEN'] = original;
      }
    }
  });

  test('scrubTokens: short token values (<=4 chars) are NOT scrubbed (guard prevents word corruption)', () => {
    const original = process.env['GH_TOKEN'];
    process.env['GH_TOKEN'] = 'ab';
    try {
      const message = 'error message with ab in it';
      expect(scrubTokens(message)).toBe(message); // unchanged — guard prevents short replacements
    } finally {
      if (original === undefined) {
        delete process.env['GH_TOKEN'];
      } else {
        process.env['GH_TOKEN'] = original;
      }
    }
  });

  // CLI-06: ANSI stripping via handler
  test('AskToolHandler: ANSI escape codes in stdout are stripped from response', async () => {
    // stdout contains ANSI red color sequence around the response text
    const ansiStdout = '\u001B[31mHello from Copilot\u001B[0m';
    mockedExecuteCommand.mockResolvedValue({ stdout: ansiStdout, stderr: '' });
    const handler = new AskToolHandler();
    const result = await handler.execute({ prompt: 'test' });
    const text = result.content[0].type === 'text' ? result.content[0].text : '';
    expect(text).toBe('Hello from Copilot');
    // ANSI escape sequence must not appear in output
    expect(text).not.toContain('\u001B');
  });

  // SuggestToolHandler and ExplainToolHandler: verify they also classify errors
  test('SuggestToolHandler: auth error produces "authentication failed" classification', async () => {
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError('copilot', 'Command failed with exit code 401: unauthenticated', new Error('auth'))
    );
    const handler = new SuggestToolHandler();
    await expect(handler.execute({ prompt: 'list files' })).rejects.toThrow(ToolExecutionError);
    try {
      await handler.execute({ prompt: 'list files' });
    } catch (e) {
      expect((e as Error).message).toMatch(/authentication failed/i);
    }
  });

  test('ExplainToolHandler: quota error produces "quota exceeded" classification', async () => {
    mockedExecuteCommand.mockRejectedValue(
      new CommandExecutionError('copilot', 'Command failed with exit code 402: rate limit exceeded', new Error('quota'))
    );
    const handler = new ExplainToolHandler();
    await expect(handler.execute({ command: 'ls -la' })).rejects.toThrow(ToolExecutionError);
    try {
      await handler.execute({ command: 'ls -la' });
    } catch (e) {
      expect((e as Error).message).toMatch(/quota exceeded/i);
    }
  });
});
```

Implementation notes:
- The existing `mockedExecuteCommand` declaration at the top of the file is already typed; the new tests reuse it without redeclaration
- `CommandExecutionError` is imported from `../errors.js` — add to the existing import line
- `scrubTokens` is also imported from `../errors.js` — add to the same import
- Each two-call test pattern (first `await expect(...).rejects.toThrow(ToolExecutionError)`, then `try/catch` to inspect message) matches the existing project test style seen in the research doc; the mock needs to be called twice so `mockedExecuteCommand.mockRejectedValue` fires for both calls
- Restore `process.env['GH_TOKEN']` in a `finally` block to avoid test pollution
  </action>
  <verify>Run `npm test` — all tests must pass. Check that the new describe block runs: `npm test -- --verbose 2>&1 | grep "Phase 2"` must show the new describe block header.</verify>
  <done>
    - `src/__tests__/error-scenarios.test.ts` has a new `describe('Phase 2: Hardened Error Handling', ...)` block
    - 9 new test cases covering ENOENT, quota, auth, timeout, token scrubbing (2 tests), ANSI, SuggestToolHandler auth, ExplainToolHandler quota
    - `npm test` passes all 42+ pre-existing tests plus the 9 new ones (51+ total)
    - `npm test -- --verbose 2>&1 | grep "Phase 2"` outputs the describe block name
  </done>
</task>

<task type="auto">
  <name>Task 2: Final test suite run and Phase 2 verification</name>
  <files></files>
  <action>
Run the full test suite and verify all Phase 2 requirements are met via grep checks.

Execute in order:

1. Full test run:
   ```bash
   npm test
   ```
   All tests must pass. Note the total test count (should be 51+).

2. Build check:
   ```bash
   npm run build
   ```
   Must exit 0 with zero TypeScript errors.

3. Requirement verification greps:
   ```bash
   # CLI-05: non-zero exit classification in handlers
   grep -n "classifyCommandError" src/tools/handlers.ts

   # CLI-06: ANSI stripping
   grep -n "stripVTControlCharacters" src/tools/handlers.ts

   # SEC-01: token scrubbing in errors.ts and command.ts
   grep -n "scrubTokens" src/errors.ts
   grep -n "scrubTokens" src/utils/command.ts

   # SEC-03: COPILOT_BINARY_PATH per-call lookup
   grep -n "COPILOT_BINARY_PATH" src/tools/handlers.ts

   # ERR-01: ENOENT detection
   grep -n "ENOENT" src/utils/command.ts

   # ERR-04: timeout in spawn
   grep -n "timeout" src/utils/command.ts

   # No hardcoded 'copilot' remaining in handler execute calls
   grep -n "executeCommand('copilot'" src/tools/handlers.ts
   ```
   The last grep must return zero matches (all three handlers use `getCopilotBinary()`).

4. If any grep returns zero matches where it should return matches, or if `executeCommand('copilot'` still appears, inspect the relevant file and fix before completing.
  </action>
  <verify>`npm test` passes; `npm run build` exits 0; all 8 greps return expected results (7 with matches, 1 with zero matches).</verify>
  <done>
    - `npm test` passes all 51+ tests
    - `npm run build` exits 0
    - All requirement greps confirm implementation
    - Zero hardcoded 'copilot' strings in handler execute calls
    - Phase 2 is complete and verifiable
  </done>
</task>

</tasks>

<verification>
Full phase verification:

```bash
npm run build   # Must exit 0
npm test        # Must pass all 51+ tests

# Spot-check requirement coverage:
grep -c "scrubTokens" src/errors.ts        # >= 2 (definition + use in handleError)
grep -c "ENOENT" src/utils/command.ts      # >= 1
grep -c "SIGTERM" src/utils/command.ts     # >= 1
grep -c "getCopilotBinary" src/tools/handlers.ts  # >= 4 (definition + 3 uses)
grep -c "Phase 2" src/__tests__/error-scenarios.test.ts  # >= 1
```
</verification>

<success_criteria>
- All 9 new test cases in `describe('Phase 2: Hardened Error Handling', ...)` pass
- Total test suite passes (51+ tests, zero failures)
- Build clean (zero TypeScript errors)
- Every requirement ID covered:
  - CLI-05: classifyCommandError() in all three AI handler catch blocks (tested via quota/auth tests)
  - CLI-06: stripVTControlCharacters() in extractResponse() (tested via ANSI test)
  - SEC-01: scrubTokens() in handleError() and on stderr log line (tested via scrubTokens tests)
  - SEC-03: getCopilotBinary() per-call lookup (grep confirms, integration tested via COPILOT_BINARY_PATH in error message)
  - ERR-01: ENOENT detection with install instructions (tested via ENOENT test)
  - ERR-02: quota classification (tested via quota tests for Ask + Explain)
  - ERR-03: auth classification (tested via auth tests for Ask + Suggest)
  - ERR-04: timeout detection (tested via timeout test)
  - TEST-03: error-scenario tests exist and pass for ENOENT, quota, auth scenarios
</success_criteria>

<output>
After completion, create `.planning/phases/02-error-handling-and-resilience/02-02-SUMMARY.md`
</output>
